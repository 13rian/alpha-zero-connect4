\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1.0in]{geometry}
\title{Alpha Zero for Connect4}
\begin{document}
\maketitle


\section{AlphaZero}
\subsection{Monte-Carlo Tree Search (MCTS)}
\subsubsection{Upper Confidence Bound}
$ U(s,a) = Q(s,a) + \sqrt{\frac{2\ln{\sum\nolimits_{b}N(s,b)}}{1 + N(s,a)}}$ \\

\noindent
$U(s,a)$ is the upper confidence bound for the current state $s$ and action $a$ \\
$Q(s,a)$ is the expected reward by taking action $a$ in state $s$ \\
$N(s,a)$ is the number of times we took action $a$ from state $s$ \\
$\sum\nolimits_{b}N(s,b)$ is the total number of plays from state $s$ \\
\\


\subsubsection{Upper Confidence Bound Alpha Zero}
\noindent 
$ U(s,a) = Q(s,a) + c_{puct} P(s,a) \frac{\sqrt{\sum\nolimits_{b}N(s,b)}}{1 + N(s,a)}$ \\

\noindent 
$U(s,a)$ is the upper confidence bound for the current state $s$ and action $a$. \\
$Q(s,a)$ is the expected reward by taking action $a$ in state $s$. \\
$c_{puct}$ is a constant that controls the amount exploration \\ 
$P(s,a)$ probability to take action $a$ in state $s$ as predicted by the neural network \\
$N(s,a)$ is the number of times we took action $a$ from state $s$ \\
$\sum\nolimits_{b}N(s,b)$ is the total number of plays from state $s$ \\
\\


\subsubsection{Training Loss}
\noindent 
$l=(z-v)^2 - \pi^T $log$ p$ \\

\noindent
$z $ is the outcome of the game -1, 0, 1 for the current player \\
$v $ is the value prediction of the value \\
$\pi $ is the policy form the MCTS \\
$p $ is the network prediction of the policy \\
\\
\pagebreak


\subsubsection{Alpha Zero Algorithm}
\textbf{while} current iteration $<$ iterations \textbf{do} \\ 
\indent \textbf{for} episode 1, M \textbf{do} \\
\indent \indent \textbf{while} !game terminated \textbf{do} \\
\indent \indent \indent \textbf{while} current simulation $<$ mctssimulations \textbf{do} \\ 
\indent \indent \indent \indent \textbf{while} !$s$ leaf node \textbf{do} \\
\indent \indent \indent \indent \indent \textbf{if} $s$ root node \textbf{then} \\
\indent \indent \indent \indent \indent \indent $p(s) = (1-\epsilon) p(s) + \epsilon \eta_d(\alpha)$ \\
\indent \indent \indent \indent \indent play move $a = $ argmax$_a \left( Q(a,s) + c_{puct} p(s,a) \frac{\sqrt{(N(s))}}{1 + N(s,a)} \right)$ \\
\indent \indent \indent \indent \indent $N(s) \leftarrow N(s) + 1$ \\
\indent \indent \indent \indent \textbf{if} $s$ terminal game state \textbf{then} \\
\indent \indent \indent \indent \indent $v \leftarrow z$ \\
\indent \indent \indent \indent \textbf{else} \\
\indent \indent \indent \indent \indent evaluate s with the network to get $v(s) and p(s)$ \\
\indent \indent \indent \indent \indent $v \leftarrow v(s)$ \\
\indent \indent \indent \indent \indent \textbf{if} player BLACK \textbf{then} \\
\indent \indent \indent \indent \indent \indent $v \leftarrow -v$ \\
\indent \indent \indent \indent \textbf{for} all state-action pairs $(s,a)$ \textbf{do} \\
\indent \indent \indent \indent \indent \textbf{if} player BLACK \textbf{then} \\
\indent \indent \indent \indent \indent \indent $v \leftarrow -v$ \\
\indent \indent \indent \indent \indent $Q(s,a) \leftarrow \frac{N(s,a) Q(s,a) + v}{N(s,a) + 1}$ \\
\indent \indent \indent \indent \indent $N(s,a) \leftarrow N(s,a) + 1$ \\
\indent \indent $p(s,a) = \left( \frac{N(s,a)}{N(s)} \right)^{1/\tau}$ \\
\indent \indent sample from $p(s)$ to play next self-play move a \\
\indent \indent add training example $(s, p(s), v')$ to experience buffer \\
\indent \indent get the true outcome $z$ of the game \\
\indent \indent \textbf{for} all training examples of game \textbf{do} \\
\indent \indent \indent \textbf{if} player WHITE \textbf{then} \\
\indent \indent \indent \indent $v' \leftarrow z$ \\
\indent \indent \indent \textbf{else} \\
\indent \indent \indent \indent $v' \leftarrow -z$ \\ 
\indent train the neural network with the training examples from the experience buffer \\


\pagebreak

\end{document}
